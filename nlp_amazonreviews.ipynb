{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4ce5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c1d6affb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644243be40ce46c3bb4f61f469e7a6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/13.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_reviews_multi (C:/Users/qyw03/.cache/huggingface/datasets/amazon_reviews_multi/zh/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029540a936424433b14e2d6db00ef890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"amazon_reviews_multi\", \"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2b507e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_id': Value(dtype='string', id=None), 'product_id': Value(dtype='string', id=None), 'reviewer_id': Value(dtype='string', id=None), 'stars': Value(dtype='int32', id=None), 'review_body': Value(dtype='string', id=None), 'review_title': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'product_category': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962cb26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a787d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2769, 1762, 6756, 4638, 4638, 4638, 677, 7481, 4638, 4638, 4638, 677, 8024, 1825, 754, 1394, 4415, 8024, 4802, 6371, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('我在车的的的上面的的的上，基于合理，确认'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc11eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 在 车 础 上 ， 基 于 合 理 ， 确 认 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([101, 1762, 6756, 4794, 677, 8024, 1825, 754, 1394, 4415, 8024, 4802, 6371, 102]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "facdf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc52dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = dataset['train']['review_body'][0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82bafb5a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_id': ['zh_0626061', 'zh_0713738', 'zh_0621612', 'zh_0757997', 'zh_0086548', 'zh_0426006', 'zh_0631437', 'zh_0638999', 'zh_0503521', 'zh_0918168', 'zh_0951339', 'zh_0920938', 'zh_0342406', 'zh_0235252', 'zh_0828230', 'zh_0558680', 'zh_0736728', 'zh_0757729', 'zh_0653472', 'zh_0987001', 'zh_0405001', 'zh_0477708', 'zh_0285038', 'zh_0534333', 'zh_0990423', 'zh_0386847', 'zh_0678100', 'zh_0240791', 'zh_0498209', 'zh_0366485', 'zh_0705691', 'zh_0848958', 'zh_0895109', 'zh_0383189', 'zh_0932306', 'zh_0719563', 'zh_0657225', 'zh_0829892', 'zh_0430443', 'zh_0893954', 'zh_0363304', 'zh_0374806', 'zh_0927934', 'zh_0231161', 'zh_0625328', 'zh_0762243', 'zh_0928808', 'zh_0729215', 'zh_0503286', 'zh_0524181', 'zh_0602855', 'zh_0412675', 'zh_0574133', 'zh_0879759', 'zh_0746712', 'zh_0658194', 'zh_0339969', 'zh_0608873', 'zh_0774556', 'zh_0459896', 'zh_0425221', 'zh_0989109', 'zh_0755375', 'zh_0452472', 'zh_0015542', 'zh_0847610', 'zh_0497022', 'zh_0138319', 'zh_0046722', 'zh_0227717', 'zh_0221325', 'zh_0777360', 'zh_0464432', 'zh_0801080', 'zh_0216681', 'zh_0307523', 'zh_0860181', 'zh_0502524', 'zh_0079304', 'zh_0709944', 'zh_0031872', 'zh_0729837', 'zh_0818062', 'zh_0116555', 'zh_0528970', 'zh_0506925', 'zh_0211748', 'zh_0281173', 'zh_0138832', 'zh_0091752', 'zh_0699052', 'zh_0644994', 'zh_0706550', 'zh_0176605', 'zh_0633301', 'zh_0794490', 'zh_0464543', 'zh_0198222', 'zh_0721211', 'zh_0795411'], 'product_id': ['product_zh_0691762', 'product_zh_0123483', 'product_zh_0670618', 'product_zh_0379151', 'product_zh_0280958', 'product_zh_0568634', 'product_zh_0326181', 'product_zh_0808561', 'product_zh_0352233', 'product_zh_0156515', 'product_zh_0590253', 'product_zh_0143371', 'product_zh_0338530', 'product_zh_0556384', 'product_zh_0893844', 'product_zh_0887673', 'product_zh_0311790', 'product_zh_0672541', 'product_zh_0835029', 'product_zh_0251234', 'product_zh_0621604', 'product_zh_0463998', 'product_zh_0334001', 'product_zh_0124233', 'product_zh_0470889', 'product_zh_0083343', 'product_zh_0126816', 'product_zh_0694868', 'product_zh_0411975', 'product_zh_0595005', 'product_zh_0378609', 'product_zh_0555159', 'product_zh_0181368', 'product_zh_0319129', 'product_zh_0440006', 'product_zh_0916982', 'product_zh_0826597', 'product_zh_0572959', 'product_zh_0953132', 'product_zh_0895698', 'product_zh_0152606', 'product_zh_0557520', 'product_zh_0703166', 'product_zh_0710699', 'product_zh_0608629', 'product_zh_0099009', 'product_zh_0873660', 'product_zh_0828329', 'product_zh_0252365', 'product_zh_0888642', 'product_zh_0361460', 'product_zh_0250315', 'product_zh_0285923', 'product_zh_0475715', 'product_zh_0916047', 'product_zh_0666599', 'product_zh_0688558', 'product_zh_0396849', 'product_zh_0373671', 'product_zh_0054023', 'product_zh_0164645', 'product_zh_0834414', 'product_zh_0354239', 'product_zh_0483486', 'product_zh_0403591', 'product_zh_0273812', 'product_zh_0219312', 'product_zh_0036302', 'product_zh_0377211', 'product_zh_0563997', 'product_zh_0587887', 'product_zh_0234937', 'product_zh_0894746', 'product_zh_0649616', 'product_zh_0028348', 'product_zh_0091748', 'product_zh_0580739', 'product_zh_0073590', 'product_zh_0735165', 'product_zh_0069239', 'product_zh_0777127', 'product_zh_0007467', 'product_zh_0213278', 'product_zh_0560844', 'product_zh_0068862', 'product_zh_0512858', 'product_zh_0140242', 'product_zh_0334996', 'product_zh_0278709', 'product_zh_0873340', 'product_zh_0472779', 'product_zh_0966185', 'product_zh_0686585', 'product_zh_0438274', 'product_zh_0558205', 'product_zh_0297957', 'product_zh_0028217', 'product_zh_0625349', 'product_zh_0109425', 'product_zh_0504310'], 'reviewer_id': ['reviewer_zh_0824776', 'reviewer_zh_0518940', 'reviewer_zh_0040023', 'reviewer_zh_0794363', 'reviewer_zh_0726131', 'reviewer_zh_0062325', 'reviewer_zh_0207055', 'reviewer_zh_0183518', 'reviewer_zh_0889667', 'reviewer_zh_0956291', 'reviewer_zh_0727873', 'reviewer_zh_0931430', 'reviewer_zh_0431334', 'reviewer_zh_0681335', 'reviewer_zh_0913275', 'reviewer_zh_0424497', 'reviewer_zh_0974511', 'reviewer_zh_0018168', 'reviewer_zh_0070627', 'reviewer_zh_0906092', 'reviewer_zh_0585107', 'reviewer_zh_0843178', 'reviewer_zh_0065778', 'reviewer_zh_0673913', 'reviewer_zh_0441345', 'reviewer_zh_0627307', 'reviewer_zh_0013371', 'reviewer_zh_0623798', 'reviewer_zh_0286510', 'reviewer_zh_0812475', 'reviewer_zh_0203971', 'reviewer_zh_0482213', 'reviewer_zh_0600753', 'reviewer_zh_0831371', 'reviewer_zh_0153240', 'reviewer_zh_0629714', 'reviewer_zh_0520894', 'reviewer_zh_0056873', 'reviewer_zh_0779284', 'reviewer_zh_0516861', 'reviewer_zh_0278205', 'reviewer_zh_0160256', 'reviewer_zh_0899378', 'reviewer_zh_0512445', 'reviewer_zh_0958656', 'reviewer_zh_0219460', 'reviewer_zh_0373702', 'reviewer_zh_0035509', 'reviewer_zh_0471257', 'reviewer_zh_0173044', 'reviewer_zh_0664987', 'reviewer_zh_0981849', 'reviewer_zh_0819156', 'reviewer_zh_0049159', 'reviewer_zh_0438584', 'reviewer_zh_0421338', 'reviewer_zh_0057119', 'reviewer_zh_0222691', 'reviewer_zh_0587471', 'reviewer_zh_0793199', 'reviewer_zh_0188503', 'reviewer_zh_0457416', 'reviewer_zh_0448686', 'reviewer_zh_0040123', 'reviewer_zh_0863288', 'reviewer_zh_0032347', 'reviewer_zh_0132783', 'reviewer_zh_0852047', 'reviewer_zh_0063984', 'reviewer_zh_0450426', 'reviewer_zh_0753444', 'reviewer_zh_0948111', 'reviewer_zh_0530942', 'reviewer_zh_0414507', 'reviewer_zh_0410739', 'reviewer_zh_0407455', 'reviewer_zh_0378472', 'reviewer_zh_0383205', 'reviewer_zh_0032513', 'reviewer_zh_0546507', 'reviewer_zh_0760269', 'reviewer_zh_0266969', 'reviewer_zh_0464492', 'reviewer_zh_0351351', 'reviewer_zh_0033905', 'reviewer_zh_0214885', 'reviewer_zh_0435395', 'reviewer_zh_0394145', 'reviewer_zh_0084860', 'reviewer_zh_0256266', 'reviewer_zh_0697762', 'reviewer_zh_0587678', 'reviewer_zh_0906215', 'reviewer_zh_0162921', 'reviewer_zh_0481031', 'reviewer_zh_0932226', 'reviewer_zh_0524212', 'reviewer_zh_0632840', 'reviewer_zh_0666335', 'reviewer_zh_0730686'], 'stars': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'review_body': ['本人账号被盗，资金被江西（杨建）挪用，请亚马逊尽快查实，将本人的200元资金退回。本人已于2017年11月30日提交退货申请，为何到2018年了还是没解决？亚马逊是什么情况？请给本人一个合理解释。', '这简直就是太差了！出版社怎么就能出版吗？我以为是百度摘录呢！这到底是哪个鱼目混珠的教授啊？！能给点干货吗？！总算应验了一句话，一本书哪怕只有一句花你感到有意义也算是本好书。哇为了找这本书哪怕一句不是废话的句子都费了我整整一天时间。。', '购买页面显示1～2日发货，付款之后显示半个月后送达，实际收到商品距离下单日期已经一个多月。 无力吐槽，一星是给商品的。', '音箱播放时断断续续的！质量完全不行，第一次在亚马逊买东西，晕！怎么是这样的呀？有客服和我联系吗？', '字太小啦，建议买别的版本，慎买呀，亲们，我后悔买了这个版本！！！', '不能转向，倒车灯不亮，转向指示灯不亮，换一次货感觉质量更差。有问题的商品亚马逊为什么还在卖，自己要砸招牌么！非常不满意的一次购物。', '爱贝尔金属边框是我见过的最好的，高大上的产品，用料足做工精美，边框在手机后背突出有效防止划伤背壳，本该满分。只给一分完全因为亚马逊自己的物流。货到山东章丘你延迟配送我可以忍，你不送到村里我不忍，为什么苏宁京东送货上门亚马逊不行？下单时你也没提示超范围，垃圾的亚马逊山东章丘配送站，亚马逊脸让你丢光了！这是第一次亚马逊购物也是最后一次亚马逊购物全因为山东章丘配送站。闹心！', '不好用，第一次就坏了，散架了。完全散架了。', '书里说有赠送CD，可是我们收到的没有，还找不到客服人员', '鞋子有质量问题，售后态度真不是一般差，客服说话很不客气。反映质量问题后也不主动让我拍照发到邮箱，反倒我主动要求发照片。寄到国外要垫付200多运费，不退货退20%货款，如果不是老公觉得退货要二十几天这么麻烦，我不可能花三百多买这么个残次品。', '很差的一本书，内容空泛，有的章节还文不对题', '收到的包装破损，书上有污泥，提交退货好几天了，至今不退款且不见卖家联系！', '只有6000毫安左右，还不给我开发票，严重质疑产品的真实性', '如果可以一颗星不给我绝对给零分，质量差手感差，料子差，而且切割不齐，明显歪扭。郁闷的是我还一下买了两个，还送人，这叫人怎么送。', '座椅靠背调节很松！座椅调节角度不太理解！是在两个角度之间任意调节，还是只能固定在这两个角度呢？靠背怎么调节', '整本书没什么层次感，内容比较散，没什么实质性的东西。', '1、吊牌价格是78元，你大耶的为啥秒杀价格比吊牌价高？ 2、原价248元，优惠价78元是从何而来？ 3、要不是因为包装直接扔了肯定要退货，卖家这样标价格忒不厚道。', '截取四五个案例，分析的有点泛泛而谈，，，', '吊牌上写的18L，标题是20.5L？？？', '我是亚马逊的忠实顾客群中的一员，虽然某宝在中国比较流行，但是我一直认为亚马逊不会有假货，虽然东西贵一点。本着这个思想，我一直在亚马逊购物，还一直选择亚马逊自营产品购买！作为一个妈妈，母婴用品在亚马逊也是我的首选，我不知道是的对亚马逊的信任，还是我的大意，我一直没有怀疑过会有假货，也一直觉得都用的很好，直到这一次的购物。 2015年3月30日，订单编号 C03-4510442-8561649，我在亚马逊购买婴儿花王纸尿裤一包，到货后我打开包装，惊奇的发现比以前在亚马逊买的都要薄，正好上次（3月15日订单编号 C03-6109563-8124044）买的一包还剩余一片，我就拿过来比较一下，果然这次买的要薄很多，另外棉质很不均匀，有些地方整个就没有夹层，我以为不是一个供货商，检查了一下，都是东莞的同一个地方！我有觉得会不会是不同的批次，但是遗憾的是还是同一批次！（照片为证）现在这种情况只有2个可能，两包都是假货，或者其中一包是假货，我不得不承认我信任的亚马逊也有假货，我自以为比麦芽宝贝，比淘宝，比京东，比其它网站都要贵一点的亚马逊，不会有假货的亚马逊也卖了假货。 接着就进入了无比生气的维权阶段，无论我是打电话投诉，还是写邮件，没有人理你，说好的回访，也没有回访，最后的说法是让我自己找供货商，我表示我没办法找供货商，我在亚马逊买的亚马逊配送的物品，怎么找供货商？那么亚马逊在我购物过程中扮演了什么角色？最后的最后亚马逊客服给我的回话是，随便我上哪儿投诉去吧，他们是不管这个事情！！！！ 我现在还在打12315，我会一直一直打，或者我实在没有精力了，或者这个事情会有合理的解决办法，但是亚马逊，我不会再来买东西了。 最后，各位亲，我朋友从日本回来给我人肉背回花王纸尿裤，我可以担保我在这里买的每一包都是假的！ 心疼各位宝妈的那颗给宝宝最好的心，也心疼宝宝们的屁屁！', '印刷质量太差～图书内页竟然有未裁开的部分～图文内容也不详细～买完之后我就感觉只能用来垫桌脚。', '很空洞，名不符实，跟写作文一样在描述，而没有知识结构的架构', '这件衣服面料的异味非常严重，类似于那种劣质衣服的甲醛味，已经洗了两次了，异味一点都没有减轻。', '不好吃，里面不知道加的什么东西，很甜，味道也不是很好，根本比不上稻香村的点心，对得起“北京御食园”这几个字吗？', '不是美亚自营的啊，中亚的海外购把美亚的第三方也归为美亚自营的了，感觉质量没保证', '很好用，买来给老人用的，速度很快，屏幕很好，声音不是很大', '感觉很难吃，买之前没看评论，不然肯定不会买了。', '亚马逊上的书现在越来越不行了，买了很多次都是盗版，这次又是，本来在亚马逊上买书看中的是亚马逊的品牌，也习惯了，没想到这么多次都是盗版，有时回来的书还有破损，想想几十块就算了，结果每次都是这样，真是让人心塞！', '１月３０就该收到货了，到现在还没有收到。', '除了前挡有用，其他什么也没有用，没几天就坏了。前挡规格太大，每次用都不好放', '东西给过来是坏的，耳机右边经常失灵听不到，烦不烦，要求退货了，退货不成功必投诉，烦死了，退货还这么慢，现在都还没上门取货，专买瑕疵的网站', '超级垃圾，发货发了半个月，多次催才到，而且来到好多都是烂的…永不会再买！！！！体验超级垃圾，超级垃圾，超级垃圾！！重要的事情说三遍…', '没有迪士尼标记，身体、头的比例不协调，包装就是一个塑料袋没有盒子，也没有生产厂商，总之很后悔、失望', '裤子款式和质量都不是很好，跟广告的图片不一样', '刚买两个月还没怎么用，屏幕灯坏了，无法退换，泪。。。', '尺寸根本不标准，又短又胖，退货还要自己寄到海外去，坑爹啊，扔了', '太坑人了，我买的书一周都没收到货，点击退货还不退给我钱，有这么坑人的商家没，可恶。', '真心很失望 亚马逊也卖盗版书？！从73到102页居然印了两遍 重复印刷这个真的不是盗版书吗？希望能给我一个解释', '首先可以确定 这个帽子和瑞士没什么关系 质量一般 肯定不值这个价', '光盘折了，包装上写的是北图的电话，联系不上，其实是北发的范围，联系北发客服只是记下单号，再没消息', '本来非常信任的，谁知道第一次在这里买书竟然就碰上了盗版，书页质量不好不说，中间还缺了几十页，全是空白，这卖家是想让我自由发挥？？？是不是真题放在其次，可这资料本身已经让人大跌眼镜，真是不敢恭维', '塑料的，两个包装都没有，味道也很大，当初眼瞎没看清是塑料，100多买个破玩意回来', '快递真是无语了，整整六天才到，我又不是在山沟沟里，而且快递公司的包装也没有，只有一个中公的袋子，这种包装还有隐私可言吗', '不知道是翻译水平的原因, 还是原作者思维的超级跳跃性, 亦或者是我本人的笨拙性, 读起这本书来, 感觉就像看无厘头的电影似的..语句东一榔头, 西一棒槌的..没有什么很强的逻辑感觉...看得非常费劲, 或者是看得非常昏昏欲睡. 后来跳出作者描述的具体细节的束缚, 看个轮廓什么的, 貌似了解到了一点东西..对我想要探究独居生活的初衷有了那么一点帮助.. 但是总的来说, 帮助不多, 性价比很糟糕..不值得推荐', '都是2014年的，全部作废，又得重新买2015年的，再不会来你家买书。', '今天就是2016年11月25日，也没见黑五便宜啊。', '不好用，没有3个月就坏掉了，只能听到对方的声音，对方听不到我们不的声音。', '林语堂通过极尽贬低王安石来反衬苏轼的伟大，主观性太强了，根本就一苏轼的粉丝，关键这B还有文化，流氓会武术谁也挡不住，文人的笔丑陋一面全暴露出来了', '为什么这书是从23页开始？这书的第一章呢？请给我一个合理的解释！！！', '除了一个裸瓶瓶外，没有任何包装，真奇葩！！！！！！没有任何产地、日期的标注，难道这是山寨货吗？？？？！', '23号买的，前几天只能听一直耳朵了，今天另一只也坏掉了，呵呵。', '东西还没拿到，但圆通居然不送货上门，明确要求需要去他们的点自提，彻底无语.', '除了后盖和表链，其它都是塑料的，特别是表壳是塑料的，第一次见。不好退货，当玩具了。表链长度也不能调。比家里一个国产的1百多块的依波表差远了，不是一个档次的。', '只能说非常失望。铺在桌子上，轻轻碰一下，便移动厉害，根本无法使用。最后，只能当一次性桌布使用了，昂贵的一次性桌布。唯一的优点是容易擦干净。但已经没有意义了。', '通篇累述，看看前几页，后面随便翻翻就行。', '之前一直很信任亚马逊上的东西，这次没想到会是这样，包装烂成这样，防伪标签明显被拆开过，实在不知道营养液有没有被打开过，万一过期了或者打开污染了就麻烦了，而且还是给孕妇吃的东西！必须退货！', '看到80页左右后面只能显示一个相机和感叹号。', '9.99，抢购的，结果当时显示交易成功，第三天显示发货，第五天却直接退钱给我了。订单取消。说是没货。', '书皮跟内页是分开的，分成了3块，换货后依然这样，不懂是不是就是这种装订方式', '已经在他们家买了几次了，书的内容本身很好没问题，但是发货特别慢，快递特别慢，每次都要四五六天才能收到，之前也没啥太在意，但次数多了，真是烦死了，真是操他妈，做什么生意，妈的，那么不道德', '没货发给我，还摆上货架，我等了一个月才告诉我没货，差评', '实际内容没有2017年下半年英语四级真题，且未开具发票。', '同时购买的为孩子立界限，书看起来很旧，像是翻阅了好久，书角也都被蹭破，看了很上火！', '1星都不想给，当看到商标没拆可退换货，有什么感受。让先写大小才能评论的设置真是智障。', '你们就是不好，还不敢让别人评论。你们网站怎么没被投诉。我真是无语。我平常网上购买很多商品，从来没有给过这样的差评。太气人呢。我在京东，苏宁，一号店，淘宝每月购买接近万吧。从来没有过差评，就能你们网站我这样生气的。做得好才有回头客生意，你们网站我无法说了。', '伪劣产品,希望亚马逊能把持好商品的质量关。', '和图片上的根本不一样 质量也很一般 总之不太满意', '号称是海外购，其实和美国买回来的完全不一样，火柴盒标志性的外包装都没有，就一个简易的外箱，而且磨损程度令人发指。本来打算圣诞节送礼的，就这包装根本没法送', '本人买了2件，发现褪色严重，掉毛也很厉害。申请一次退货，未回复。第二次申请，卖家说已经过了3个月了，没有办法处理了。', '为啥买了两本书，只收到一本，订单显示已签收。我买的是套装，为啥只有一本，现在还没人理', '我秒杀的1条。为什么给我发2条？还不能退？', '说实话吧，现在觉得，不看这类成功学，多去做事，你就会比看成功学之后，要成功', '教科书。 买回来之后发现和同学在学校订的课本的厚度是不一样的，明显的盗版。 只有同学课本的一半厚多一些吧！ 之前都是来亚马逊买书的，以后大概不会再来了！！', '完全是西方人的视角，有偏见！部分文字甚至有对领袖的诋毁！', '寄过来的盖子都是坏的，喷头碎的和瓶子分家了', '垃圾快递，明明写着早早就拿到货了，就是给送，真不敢恭维亚马逊的办事效率。', '口口声声说原装，首先用了不到一个月裂了，崩开一个口子，背面的字几天就掉了，我看中的是亚马逊，哪知道这地方的假货反而更多！', '打气麻烦，没有图片好看，而且收到的是有瑕疵的，巴掌大的污迹，但是海淘基本不考虑退货，所以差评', '收到货后，打开一看，令我大失所望，正面一层薄薄的人造革，背面无纺布，做工粗糙，便宜没好货，说的真没错啊，那么多好评怎么来的，我都纳闷了？', '无语了 看着买的人评论的人挺多的 而且名字和封面《封面这句话挺吸引人的》 就买了 看过后 《实在看不下去了》 觉得这是我从小到大买过的最垃圾的一本书 悲催 这也能出书？ 唉... 劝准备买的人 无论是什么目的 是什么职业 年龄多大 最好不要买 除非你有钱 任性！', '口罩没有呼吸阀，戴起来很不方便，尤其是冬天。亚马逊第三方，卖家的行为很low，双十一购入后迅速降价，一度降到90出头的价格，卖家还表示是在搞活动， ……搞活动要在双十一之后迅速降价，这样的促销思路真是异于常人。另，卖家沟通能力堪忧。希望给后面的买家一个提醒，避免踩雷。', '第一次亚马逊发个残品，直接退货，又买了一个，拆开包装，连原装鞋盒都没有了，居然是个亚马逊的鞋盒子，让我怎么相信鞋是真的，我是亚马逊的老客户了，但是这一次是在让我太失望了，希望客服给予回复，我会持续关注', '这个价钱，买这个灯不值得，太差了，一点都不亮，垃圾玩意.', '第一次在亚马逊上买书，拿到《理想国》好失望，书基本都散架了', '你们丰程物流怎么回事？配送不及时，打电话也不接，上次也是这样。', '操我！老公们快操我！我要吃大鸡巴！！舔你们龟头,喝你们浓精！', '说实在话，这本书基本没帮助，我是想从书里学习一下家装的配色方案，这本书第一不系统，第二举的例子太泛泛，基本没有帮助。', '以为是正常书的规格，结果就三星手机那么大，而且纸张泛黄且薄像盗版，很后悔！', '真的是中学生水准的文笔，能感觉到作者很用心，但是，水平有限啊。 另，我是看了评论才买的，这是什么网络行为艺术吗？怎么会有那么多人好评？', '不是喜欢的类型 。。。。。。。。。。。。。', 'iphone都没找到注册的按钮 体脂率明显偏高 颠覆性的偏高！app不稳定常死机 非常失望的国货产品', '亚马逊，我真醉了，第一次买东西，未接到快递一个电话，货也未看到好坏，确显示配送成功，还好是货到付款，如果是先付款，再发货，那真是自找麻烦！我倒是问问快递员是否货好，你自己要了还是掉了，总之是一种不负责任的快递，真不知道以后还相信么，耽误人家时间！', '质量太差，我妈妈使用两个月左右就开始碎裂，真不敢想象这是配给诺基亚的保护壳，脆的不行，塑料部分上下左右基本上都裂了。 就没见过这么脆的保护壳。以前买过lumia 900的保护壳，可不像这样子', '质量太差，有种上当受骗的感觉，再也不买了。', '买的37.5.给我发了36.5 我还在奇怪为什么一点都不宽松，还有些许过于挤脚，发小了一码', '很差的一款鞋，不要买！鞋底太硬，没有中底，所以没有缓震或支撑。', '买了一就没有必要买二，内容重复，而且二没有一详细。', '这个染发乳效果很差 不上色 对白头发没有效果', '不好，与我以前用的日立7.2v的相比，差的远了，没有12v力量，垃圾产品，用了就知道了。', '购买这灯开关用不到30次，就出现问题，春节期间不亮了。春节期间打电话反馈亚马逊该怎么处理，客服说春节期间放假，假期后会给我电话，尼玛等了几个月没有看到有电话联系我。之后九月份回到老家又记起这灯的问题，随后打电话厂家，厂家说保修期没过，让亚马逊处理售后，之后又打给客服，客服说像上级反馈过两天再给电话回复。等啊等，又等到国庆末期，有还是没电话。怒了，又再一次给亚马逊电话，客服说节后一定给我个满意的答复。之后NMB,MMP，电话都没有，就尼玛的这客服，这售后，老子要是在亚马逊买一分钱的东西就他妈的剁手。ps：我网购差不多十年不管品质好坏，一般不评论，都是自动好评，重来不给中评或差评过，这次亚马逊的售后和客服真是垃圾的要屎。望大家引以为鉴。'], 'review_title': ['此书不是本人购买', '简直是废话！', '最牛逼的预售', '迷你音响', '排版太密，不适合菜鸟用，看到眼睛花了！', '商品质量有问题', '金属边框是最好的，山东章丘配送站最垃圾', '不好用，第一次就坏了，散架了。', '书里说有赠送CD，可是我们收到的没有，还找不到客服人员', '售后差，鞋子是残次品', '很差的一本书', '书上有污泥，提交退货好几天了，至今不退款且不见卖家联系', '不给我开发票', '千万别买', '靠背调节！', '写的比较乱，很失望', '亚马逊你要给我个解释！', '内容也太少了，才几页...', '评论', '真想给零颗星！亚马逊，你真让买家失望！', '不值得够购买', '既然是这个概念名，起码就该是门学科知识性的架构知识，没有，跟回忆录似的描述不推荐购买', '异味严重', '不好吃', '这是美国第三方配送的', '很超值', '很难吃', '盗版盗版', '其乐男士休闲鞋', '基本没什么用', '让人看不懂，呵呵', '垃圾的东西！', 'å\\x95\\x86å®¶è¯¯å¯¼ï¼\\x8cä¸\\x8eå®\\x9eç\\x89©æ\\x9c\\x89å·®å¼\\x82', '质量款式介绍和实际的商品有差别', '刚买两个月还没怎么用，屏幕灯坏了，无法退换，泪。。。', '尺寸根本不标准，又短又胖，退货还要自己寄到海外去，坑爹啊，扔了', '骗子', '重复印刷', '质量一般', '卖家包装太差，光盘折成两截，联系客服没有反馈', '缺页！缺页！缺页！盗版！盗版！盗版！！！', '塑料的，两个包装都没有', '不愉快', '可读性不强', '过期了，很生气。', '黑色星期五哪便宜了？', '好看不实用', '主观性太强', '这是盗版书吗？', '除了一个裸瓶瓶外，没有任何包装，真奇葩！！！！！！', '马上坏', '配送太差', '质量很次', '失望', '几个观点，几页A4文档就能说清楚的事儿，弄一本书，耽误功夫！', '东西被拆开', '后几十页无法在paperwrite上看', '太过分了，欺骗消费者', '装订', '每次快递都特别慢！！！！！！操他妈', '差评，没货还上架', '投诉', '为孩子立界限的书', '不建议买', '我购买了，就应该让我评论，我又不是乱说。', '不合格产品', '不会再在这家店买第二次', '没有包装，清库存的老货', '质量有瑕疵', '缺一本古汉语', '我秒杀的1条。为什么给我发2条？还不能退？', '说实话吧', '盗版', '一般', '差评', '很不爽的一次购物经历，以后真心不考虑亚马逊了！', '纯属骗子', '品质很差', '失望', '纯粹是哗众取宠的一本无聊无味自以为很逗其实并没有任何观点任何营养的连打发时间去看都觉', '很糟糕', '没有原装鞋盒，鞋里面没有塞纸', '已经不亮了，炸了，炸了，炸了，重要的事说三遍', '理想国', '物流垃圾', '操我！老公们快操我！我要吃大鸡巴！！舔你们龟头,喝你们浓精！', '基本没帮助', '失望', '文笔太差了吧', '卸载了已经。。。。。。', '家用产品还是不够成熟', '强烈反对服务差', '烂保护壳，两三个月就碎了', '质量太差，有种上当受骗的感觉，再也不买了。', '无奈', '照片和实物差别太大', '太坑了！', '效果不好', '垃圾产品，', '垃圾亚马逊~屎一样的客服和售后！！！'], 'language': ['zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh', 'zh'], 'product_category': ['book', 'book', 'home_improvement', 'other', 'book', 'toy', 'other', 'home', 'book', 'shoes', 'book', 'book', 'wireless', 'office_product', 'baby_product', 'book', 'sports', 'digital_ebook_purchase', 'luggage', 'baby_product', 'book', 'book', 'sports', 'grocery', 'musical_instruments', 'wireless', 'grocery', 'book', 'shoes', 'automotive', 'other', 'sports', 'toy', 'apparel', 'office_product', 'apparel', 'book', 'book', 'sports', 'book', 'book', 'baby_product', 'book', 'digital_ebook_purchase', 'book', 'pc', 'wireless', 'book', 'book', 'beauty', 'other', 'toy', 'watch', 'home', 'digital_ebook_purchase', 'drugstore', 'digital_ebook_purchase', 'luggage', 'book', 'book', 'sports', 'book', 'book', 'apparel', 'baby_product', 'shoes', 'home', 'toy', 'apparel', 'book', 'apparel', 'book', 'book', 'digital_ebook_purchase', 'drugstore', 'apparel', 'wireless', 'baby_product', 'automotive', 'book', 'drugstore', 'shoes', 'automotive', 'book', 'book', 'digital_ebook_purchase', 'book', 'book', 'digital_ebook_purchase', 'digital_ebook_purchase', 'drugstore', 'drugstore', 'wireless', 'office_product', 'shoes', 'shoes', 'digital_ebook_purchase', 'beauty', 'home_improvement', 'home_improvement']}\n"
     ]
    }
   ],
   "source": [
    "raw_data = dataset['train'][0:100]\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115571ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本人账号被盗，资金被江西（杨建）挪用，请亚马逊尽快查实，将本人的200元资金退回。本人已于2017年11月30日提交退货申请，为何到2018年了还是没解决？亚马逊是什么情况？请给本人一个合理解释。\n",
      "这简直就是太差了！出版社怎么就能出版吗？我以为是百度摘录呢！这到底是哪个鱼目混珠的教授啊？！能给点干货吗？！总算应验了一句话，一本书哪怕只有一句花你感到有意义也算是本好书。哇为了找这本书哪怕一句不是废话的句子都费了我整整一天时间。。\n"
     ]
    }
   ],
   "source": [
    "print(raw_data[0])\n",
    "print(raw_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fcd6d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(raw_data, padding='max_length', truncation=True, max_length=42)#, return_tensors='pt')\n",
    "# return_tensors=\"pt\") \n",
    "# padding='max_length', truncation=True, max_length=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "45636330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids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token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4804e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2342c07c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "aa004a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.2372, -1.3714],\n",
      "        [ 2.8215, -2.7044],\n",
      "        [ 2.8547, -2.6289],\n",
      "        [ 2.7500, -2.7942],\n",
      "        [ 1.5426, -1.6068],\n",
      "        [ 2.7676, -2.6815],\n",
      "        [-1.3157,  1.2451],\n",
      "        [ 2.3486, -2.2169],\n",
      "        [ 1.8789, -1.8561],\n",
      "        [ 2.3374, -2.1531]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "52631bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = torch.softmax(output.logits, dim=1)#.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c4399c41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9314, 0.0686],\n",
      "        [0.9960, 0.0040],\n",
      "        [0.9959, 0.0041],\n",
      "        [0.9961, 0.0039],\n",
      "        [0.9589, 0.0411],\n",
      "        [0.9957, 0.0043],\n",
      "        [0.0717, 0.9283],\n",
      "        [0.9897, 0.0103],\n",
      "        [0.9767, 0.0233],\n",
      "        [0.9889, 0.0111]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d04614bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ['本人账号被盗，资金被江西（杨建）挪用，请亚马逊尽快查实，将本人的200元资金退回。本人已于2017年11月30日提交退货申请，',\n",
    "         '为何到2018年了还是没解决？亚马逊是什么情况？请给本人一个合理解释。这简直就是太差了！出版社怎么就能出版吗',\n",
    "         '？我以为是百度摘录呢！这到底是哪个鱼目混珠的教授啊？！能给点干货吗？！总算应验了一句话，',\n",
    "         '一本书哪怕只有一句花你感到有意义也算是本好书。哇为了找这本书哪怕一句不是废话的句子都费了我整整一天时间.',\n",
    "        '书很好， 服务差',\n",
    "        '我很不高兴， 太差了',\n",
    "        '强烈推荐，质量很好']\n",
    "inputto = tokenizer(input, padding='max_length', truncation=True, max_length=42, return_tensors='pt')\n",
    "output = model(**inputto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b1eaefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6dcf38d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78707451 0.99560308 0.8989805  0.44750613 0.90877485 0.99225873\n",
      " 0.03266997]\n"
     ]
    }
   ],
   "source": [
    "results = torch.softmax(output.logits, dim=1).tolist()\n",
    "print(np.array(results)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941dc973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "204171b4",
   "metadata": {},
   "source": [
    "# Build model on bert base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dbeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ee451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"amazon_reviews_multi\", \"zh\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3157673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63458798",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datase = dataset['train'][0:500]\n",
    "train_dataset_raw = train_datase['review_body']\n",
    "train_tokenized = tokenizer(train_dataset_raw, padding='max_length', truncation=True, max_length=42, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c2263b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 3315,  782,  ...,  511, 3315,  102],\n",
      "        [ 101, 6821, 5042,  ..., 3921, 4403,  102],\n",
      "        [ 101, 6579,  743,  ...,  671,  702,  102],\n",
      "        ...,\n",
      "        [ 101, 2552, 1853,  ...,    0,    0,    0],\n",
      "        [ 101, 2340, 7481,  ...,    0,    0,    0],\n",
      "        [ 101, 3905, 2140,  ..., 8043, 8013,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e1273c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc7c844",
   "metadata": {},
   "source": [
    "定义计算准确率的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b7cdb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 根据预测结果和标签数据来计算准确率\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = np.argmax(labels, axis=1).flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # 四舍五入到最近的秒\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # 格式化为 hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "256eb585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nval_dataset = dataset['validation'][0:500]\\na = np.array(val_dataset['stars'])\\nb = [[item/5, 1-item/5] for item in a]\\nval_label = torch.FloatTensor(b)\\n\\nval_dataset_raw =val_dataset['review_body']\\nval_tokenized = tokenizer(val_dataset_raw, padding='max_length', truncation=True, max_length=42, return_tensors='pt')\\nval_input = TensorDataset(val_tokenized['input_ids'], val_tokenized['attention_mask'],                          val_label)\\n\\n\""
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "a = train_dataset['stars']\n",
    "b = [[item/5, 1-item/5] for item in a]\n",
    "train_label = torch.FloatTensor(b)\n",
    "\n",
    "\n",
    "train_dataset_raw = train_dataset['review_body']\n",
    "train_tokenized = tokenizer(train_dataset_raw, padding='max_length', truncation=True, max_length=42, return_tensors='pt')\n",
    "train_input = TensorDataset(train_tokenized['input_ids'],train_tokenized['attention_mask'],\\\n",
    "                            train_label)\n",
    "\n",
    "\n",
    "#train_input = train_input[torch.randperm(n_data)]\n",
    "# 计算训练集和验证集大小\n",
    "train_size = int(0.8 * len(train_input))\n",
    "val_size = len(train_input) - train_size\n",
    "\n",
    "# 按照数据大小随机拆分训练集和测试集\n",
    "train_dataset1, val_dataset1 = random_split(train_input, [train_size, val_size])\n",
    "\n",
    "'''\n",
    "val_dataset = dataset['validation'][0:500]\n",
    "a = np.array(val_dataset['stars'])\n",
    "b = [[item/5, 1-item/5] for item in a]\n",
    "val_label = torch.FloatTensor(b)\n",
    "\n",
    "val_dataset_raw =val_dataset['review_body']\n",
    "val_tokenized = tokenizer(val_dataset_raw, padding='max_length', truncation=True, max_length=42, return_tensors='pt')\n",
    "val_input = TensorDataset(val_tokenized['input_ids'], val_tokenized['attention_mask'],\\\n",
    "                          val_label)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "db61b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 16\n",
    "# 为训练和验证集创建 Dataloader，对训练样本随机洗牌\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset1,  # 训练样本\n",
    "            sampler = RandomSampler(train_dataset1), # 随机小批量\n",
    "            batch_size = batch_size # 以小批量进行训练\n",
    "        )\n",
    "\n",
    "# 验证集不需要随机化，这里顺序读取就好\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset1, # 验证样本\n",
    "            sampler = SequentialSampler(val_dataset1), # 顺序选取小批量\n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "fe1cb304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch     0  of  10,000.    Elapsed: 0:00:00.\n",
      "  Batch     2  of  10,000.    Elapsed: 0:00:04.\n",
      "  Batch     4  of  10,000.    Elapsed: 0:00:09.\n",
      "  Batch     6  of  10,000.    Elapsed: 0:00:13.\n",
      "  Batch     8  of  10,000.    Elapsed: 0:00:17.\n",
      "  Batch    10  of  10,000.    Elapsed: 0:00:21.\n",
      "  Batch    12  of  10,000.    Elapsed: 0:00:25.\n",
      "  Batch    14  of  10,000.    Elapsed: 0:00:29.\n",
      "  Batch    16  of  10,000.    Elapsed: 0:00:33.\n",
      "  Batch    18  of  10,000.    Elapsed: 0:00:37.\n",
      "  Batch    20  of  10,000.    Elapsed: 0:00:42.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13560\\586842464.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# 更新参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# 平均训练误差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\adamw.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    146\u001b[0m                     \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                     \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                     eps=group['eps'])\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "# 我认为 'W' 代表 '权重衰减修复\"\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 5e-5)\n",
    "\n",
    "# 以下训练代码是基于 `run_glue.py` 脚本:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# 设定随机种子值，以确保输出是确定的\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "#torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 存储训练和评估的 loss、准确率、训练时长等统计指标, \n",
    "training_stats = []\n",
    "\n",
    "# 统计整个训练时长\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 统计单次 epoch 的训练时间\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 重置每次 epoch 的训练总 loss\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # 将模型设置为训练模式。这里并不是调用训练接口的意思\n",
    "    # dropout、batchnorm 层在训练和测试模式下的表现是不同的 (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # 训练集小批量迭代\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "         # 每经过40次迭代，就输出进度信息\n",
    "        if step % 2 == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "    #batch_size = 200\n",
    "    #nbatch_train = 5 # = int(500/10)\n",
    "    #for ibatch in range(nbatch_train):\n",
    "        #print(ibatch)\n",
    "       # batch = train_input[int(ibatch*batch_size) : int((ibatch+1)*batch_size)]\n",
    "        # 准备输入数据，并将其拷贝到 gpu 中\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # 每次计算梯度前，都需要将梯度清 0，因为 pytorch 的梯度是累加的\n",
    "        optimizer.zero_grad()  # 梯度清0 # model.zero_grad()        \n",
    "        # 前向传播\n",
    "        # 文档参见: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # 该函数会根据不同的参数，会返回不同的值。 本例中, 会返回 loss 和 logits -- 模型的预测结果\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask) #, labels=b_labels)\n",
    "        results = torch.softmax(output.logits, dim=1)\n",
    "        loss = criterion(results, b_labels) \n",
    "        # 累加 loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 梯度裁剪，避免出现梯度爆炸情况\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "    # 平均训练误差\n",
    "    avg_train_loss = total_train_loss / nbatch            \n",
    "    \n",
    "    # 单次 epoch 的训练时长\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # 完成一次 epoch 训练后，就对该模型的性能进行验证\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        # 准备输入数据，并将其拷贝到 gpu 中\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # 评估的时候不需要更新参数、计算梯度\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask) #, labels=b_labels)\n",
    "        results = torch.softmax(output.logits, dim=1)\n",
    "        loss = criterion(results, b_labels) \n",
    "        \n",
    "        # 累加 loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # 将预测结果和 labels 加载到 cpu 中计算\n",
    "        logits = results.detach().numpy()\n",
    "        label_ids = b_labels.numpy()\n",
    "        \n",
    "        # 计算准确率\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # 打印本次 epoch 的准确率\n",
    "    avg_val_accuracy = total_eval_accuracy / nbatch_val\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # 统计本次 epoch 的 loss\n",
    "    avg_val_loss = total_eval_loss / nbatch_val\n",
    "    \n",
    "    # 统计本次评估的时长\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # 记录本次 epoch 的所有统计信息\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e2c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10253093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc0b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "241fb2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassificationModel(nn.Module):\n",
    "    def __init__(self,hidden_size=768):\n",
    "        super(BertClassificationModel, self).__init__()\n",
    "        # 这里用了一个简化版本的bert\n",
    "        model_name = 'uer/roberta-base-finetuned-dianping-chinese'\n",
    "\n",
    "        # 读取分词器\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # 读取预训练模型\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        for p in self.bert.parameters(): # 冻结bert参数\n",
    "                p.requires_grad = False\n",
    "        self.fc = nn.Linear(hidden_size,2)\n",
    "\n",
    "    def forward(self, batch_sentences):   # [batch_size,1]\n",
    "        sentences_tokenizer = self.tokenizer(batch_sentences,\n",
    "                                             truncation=True,\n",
    "                                             padding=True,\n",
    "                                             max_length=512,\n",
    "                                             add_special_tokens=True)\n",
    "        input_ids=torch.tensor(sentences_tokenizer['input_ids']) # 变量\n",
    "        attention_mask=torch.tensor(sentences_tokenizer['attention_mask']) # 变量\n",
    "        bert_out=self.bert(input_ids=input_ids,attention_mask=attention_mask) # 模型\n",
    "\n",
    "        last_hidden_state =bert_out[0] # [batch_size, sequence_length, hidden_size] # 变量\n",
    "        bert_cls_hidden_state=last_hidden_state[:,0,:] # 变量\n",
    "        fc_out=self.fc(bert_cls_hidden_state) # 模型\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e0cd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testNumber = 5000    # 多少个数据参与训练模型\n",
    "validNumber = 5000   # 多少个数据参与验证\n",
    "batchsize = 250  # 定义每次放多少个数据参加训练\n",
    "\n",
    "trainDatas = dataset['train'] #ImdbDataset(mode=\"test\",testNumber=testNumber) # 加载训练集,全量加载，考虑到我的破机器，先加载个100试试吧\n",
    "validDatas =  dataset['validation'] #ImdbDataset(mode=\"valid\",validNumber=validNumber) # 加载训练集\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainDatas, batch_size=batchsize, shuffle=False)\n",
    "#遍历train_dataloader 每次返回batch_size条数据\n",
    "val_loader = torch.utils.data.DataLoader(validDatas, batch_size=batchsize, shuffle=False)\n",
    "epoch_num = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40d4fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型数据已经加载完成,现在开始模型训练。\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13560\\3027499177.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"模型数据已经加载完成,现在开始模型训练。\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 梯度清0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "model=BertClassificationModel()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5) # ???\n",
    "# 这里是定义损失函数，交叉熵损失函数比较常用解决分类问题\n",
    "# 依据你解决什么问题，选择什么样的损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"模型数据已经加载完成,现在开始模型训练。\")\n",
    "for epoch in range(epoch_num):\n",
    "    for i, (data,labels) in enumerate(train_loader, 0):\n",
    "        output = model(data[0])\n",
    "        optimizer.zero_grad()  # 梯度清0\n",
    "        loss = criterion(output, labels[0])  # 计算误差\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        # 打印一下每一次数据扔进去学习的进展\n",
    "        print('batch:%d loss:%.5f' % (i, loss.item()))\n",
    "\n",
    "    # 打印一下每个epoch的深度学习的进展i\n",
    "    print('epoch:%d loss:%.5f' % (epoch, loss.item()))\n",
    " #下面开始测试模型是不是好用哈\n",
    "print('testing...(约2000秒(CPU))')\n",
    "\n",
    "# 这里载入验证模型，他把数据放进去拿输出和输入比较，然后除以总数计算准确率\n",
    "# 鉴于这个模型非常简单，就只用了准确率这一个参数，没有考虑混淆矩阵这些\n",
    "num = 0\n",
    "model.eval()  # 不启用 BatchNormalization 和 Dropout，保证BN和dropout不发生变化,主要是在测试场景下使用；\n",
    "for j, (data,labels) in enumerate(val_loader, 0):\n",
    "    output = model(data[0])\n",
    "    # print(output)\n",
    "    out = output.argmax(dim=1)\n",
    "    # print(out)\n",
    "    # print(labels[0])\n",
    "    num += (out == labels[0]).sum().item()\n",
    "    # total += len(labels)\n",
    "print('Accuracy:', num / validNumber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c614f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
